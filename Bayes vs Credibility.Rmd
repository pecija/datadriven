---
title: "Bayes vs Credibility"
author: "Pedro Ecija Serrano"
date: "14 March 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Start

Loading the libraries we will need later and the package with the insurance data we will use. We are interested in the 'dataCar' dataset

```{r, warning=FALSE}
  library(MASS) #Useful to fit statistical distributions
  library(insuranceData)
  #This package comes with the following datasets:
  #AutoBi: US motor insurance claims
  #AutoClaims: US motor insurance claims
  #AutoCollision: UK motor insurance claims data
  #ClaimsLong: simulated motor insurance book (policies and claims)
  #dataCar: motor insurance claims data
  #dataOhlson: Sweedish motorcycle insurance claims data
  #IndustryAuto: US motor insurance claims triangle
  #SingaporeAuto: Singaporean motor insurance claims data
  #ThirdParty: Australian motor insurance claims data
  #WorkersComp: Workers Compensation claims data
  
  library(dplyr)
  library(ggplot2)
  data(dataCar)
  set.seed(4321)
  
```

##Number of Claims

The dataset represents the total population and our assumption is that the claims and loss distributions followed seen in the population are available as a market index.

We will also assume that an insurance company is targetting women but has no data regarding how women's claims experience may differ from the population's.

The company could follow a Bayesian approach and consider the population's distributions as priors and develop a posterior distribution with any claims experience gained during operations.

The insurer could also follow an actuarial approach and use credibility theory to develop its own distributions.

We will try both and compare results.

```{r}
  
  hist(dataCar$numclaims, col='red', xlab = 'Number of Claims')
  popclaimsn <- sum(dataCar$numclaims) #Total number of claims in the dataset
  popsize <- nrow(dataCar) #Size of the dataset
  popmean <- mean(dataCar$numclaims) #Mean
  popvar <- var(dataCar$numclaims) #Variance
  
```

The shape of the histogram, together with the similarity between the mean (`r round(popmean,4)`)  and the variance (`r round(popvar,4)`), indicate that the population's number of claims may be Poisson distributed. 

Actually, a Poisson did not seem to fit well.  A zero-inflated Poisson or Negative Binomial could perhaps do better but we will assume a Poisson anyway as it makes it easier for us now and the point of this exercise is not to fit statistical distributions to data but to compare a Bayesian approach with Credibility theory.

Let's create a subset with only female drivers.  This is the market the insurance company is targetting and whose distribution we ignore.

We will also create a random sample from this subset, which will be the company's policyholders (I arbitrarily choose a third of all female drivers).

```{r}
  set.seed(600)
  nsample <- 1/3
  femdata <- filter(dataCar, gender=='F')
  femsample <- sample_n(femdata, round(nsample*nrow(femdata),0), replace=FALSE)
  
```

There are `r nrow(femdata)` rows in the dataset and the company's portfolio has `r nrow(femsample)` policyholders.

###Bayesian approach to number of claims

Here we will see how a Bayesian approach, taking the population's distribution for the number of claims as prior distribution, produces a posterior distribution based on the data from the company's policyholders.

The Poisson's conjugate prior is a Gamma distribution with two parameters: shape and rate.  Shape equals the number of claims and rate equals the number of observations.

We know there are `r popclaimsn` in a population of `r popsize` policies. This means our prior parameters are: scale = `r popclaimsn` and rate = `r popsize`.

These parameters will be udpated with the claims experience arising from the company's book.

```{r}
  #Prior distribution
  xrange <- seq(0.05,0.1,0.0001)
  alpha <- popclaimsn #Shape
  beta <- popsize #Size
  nprior <- dgamma(xrange, shape=alpha, rate=beta)
  
  #Posterior distribution
  alpha.star <- alpha + sum(femsample$numclaims)
  beta.star <- beta + nrow(femsample)
  nposterior <- dgamma(xrange, shape = alpha.star, rate=beta.star)
  
  qplot(xrange, nprior, geom='line', color='red')+geom_line(aes(xrange, nposterior), color='blue')
  
```

The red line shows the prior conjugate, a gamma with parameters based on the population.  The blue line shows how this distribution has been affected with the data: the mean has shifted slightly to the right and the density of probability seems a little narrower, indicating more certainty over the estimated parameter.

The population's mean was `r round(alpha/beta, 4)` and the portfolio's mean is `r round(alpha.star/ beta.star, 4)`.

In terms of variance, the numbers are so small that we are better off comparing the log of the variance: the variance based on the population was `r log(alpha/beta^2)` and the variance based on the posterior distribution is `r log(alpha.star/beta.star^2)`

So, using a Bayesian approach to estimate the posterior distribution, we now believe the number of claims in the company's portfolio is a random variable Poisson distributed with mean and variance equal to `r alpha/beta`

###Credibility approach to number of claims

Credibility theory allows for the combination of old and new information with the use of a credibility factor.  This factor measures the strength of our belief in the new data so that the estimated parameter.  In our case we try to estimate the mean and variance of a Poisson distribution.

Credibility theory says that the new parameter is given by the following expresion: $\lambda' = \lambda^{*}*z + \lambda * (1-z)$  Where $\lambda'$ is the parameter we will use, $lambda$ is the original parameter from the population's distribution and $lambda^{*}$ is the parameter solely based on the company's portfolio.

The credibility factor z is calculated as: $z = \sqrt (n/N)$  Where n is the size of the portfolio and N is the size of the population.  We could use other magnitudes such as written premium rather than size, etc but it appears appropriate in this case and we do not really have much else to use.

```{r}
  lambda <- popmean
  lambda.star <- mean(femsample$numclaims)
  z <- sqrt(nrow(femsample)/popsize)
  lambda.comma <- lambda.star * z + lambda * (1-z)
  
```

The result is `r round(lambda.comma, 4)`.

Let's find a better way to compare the results we have:

```{r}
  plotdata <- c(popmean, alpha.star/beta.star, lambda.comma, lambda.star)
  plotlabels <- c('Population', 'Bayes Posteriori', 'Credibility', 'Portfolio')
  qplot(x=plotlabels, y=plotdata, geom='point')
  
```

##Claims Cost

We can follow a similar process to come out with an estimate for the company's average claims cost (the loss).  Let us see first what the population's distribution looks like:

```{r}
  hist(dataCar$claimcst0, col='red', xlab = 'Claims Cost')
  qplot(dataCar$claimcst0, geom='density', fill='Density')
  
```

A significant problem is the case where we have zero cost. This corresponds to policyholders with no claims so a better way to look at this is to remove zero cost claims and work only with non-zero cost-values.

```{r}
  CarClaims <- filter(dataCar, numclaims >0)

```

The package 'fitdistrplus' has many useful functions to fit statistical distributions that we can use.

I will save you the pain of trying to fit a distribution by saying we will take a Weibull.  Attempts showed it is not an easy task and the fits I was getting were not particularly good so, as we did with the number of claims, we will pretend the data follows a Weibull distribution.

```{r}
  library(fitdistrplus) #This packages has useful functions to fit statistical distributions.
  descdist(CarClaims$claimcst0, boot=1000) 
  #The optional boot parameter is used apply a non-parametric bootstrap procedure to take into account the uncertainty in the estimates for skewness and kurtosis, which are known not to be robust.
  
  ccost <- fitdist(CarClaims$claimcst0, 'weibull')
  summary(ccost)
  plot(ccost)
  ccost$sd
  shape2 <- ccost$sd[1]
  scale2 <- ccost$sd[2]

```

We now have a Weibull distribution with two parameters: shape = `r round(shape2, 4)` and scale = `r round(scale, 4)`.

That is the assumed distribution of claim cost for the population and the distribution we will use as prior in the Bayesian approach.  This is also used as our reference in the Credibility part.

```{r}
  #Population statistics based on non-zero claims
  popcosts <- sum(CarClaims$claimcst0) #Total claims loss
  popsizes <- nrow(CarClaims) #Size of the dataset with non-zero claims
  popmeans <- mean(CarClaims$claimcst0) #Mean
  popvars <- var(CarClaims$claimcst0) #Variance
  
  #Generating the company's book, based on non-zero claims
  FemCarClaims <- filter(femsample, numclaims>0)
  
```

###Bayesian approach to claim cost

The prior conjugate for a Weibull distribution is an inverse gamma distribution with parameters a and b calculated below.

```{r}
  xrange2 <- seq(200,56000,10)
  a <- popsizes
  b <- popcosts + sum(CarClaims$claimcst0^scale2)
  cprior <- dinvgamma(xrange2, shape=a, rate=b)
  
```

The posterior parameters are calculated based on the data taken from the company's book.

```{r}
  a.star <- a + nrow(FemCarClaims)
  b.star <- b + sum(FemCarClaims$claimcst0) + sum(FemCarClaims$claimcst0^scale2)
  cposterior <- dinvgamma(xrange2, shape=a.star, rate=b.star)

  qplot(xrange2, cprior, geom='line', color='red')+geom_line(aes(xrange2, cposterior), color='blue')
  
```



###Credibility approach to claim cost